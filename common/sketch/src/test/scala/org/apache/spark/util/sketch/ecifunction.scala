package com.optum.bds.brms.spark       2                  +       3                  +class ECIFunctions extends Serializable{       4                  +    val dateformat = new java.text.SimpleDateFormat("yyyy-MM-dd-HH:mm:ss.SSSSSS")       5                  +  def getDate(): String = {       6                  +    val calendar = java.util.Calendar.getInstance()       7                  +    val tmp = new java.sql.Timestamp(calendar.getTimeInMillis())       8                  +    return dateformat.format(tmp)       9                  +  }       10                  +  def getTimeDifference(startTime: String, endTime: String): String = {       11                  +    val t1 = dateformat.parse(startTime).getTime       12                  +    val t2 = dateformat.parse(endTime).getTime       13                  +    val timeDiff = t2 - t1       14                  +    val secDiff = timeDiff / 1000 % 60       15                  +    val minDiff = timeDiff / (60 * 1000) % 60       16                  +    val hourDiff = timeDiff / (60 * 60 * 1000) % 24       17                  +    return "Hours: " + hourDiff + ", Minutes: " + minDiff + ",Seconds: " + secDiff       18                  +  }       19                  +       20                  +  def validator(line: String, recTypExist: Seq[String]): Array[String] = {       21                  +    var returnArray = new scala.collection.mutable.ArrayBuffer[String]       22                  +    var failedOutput = ""       23                  +    var passedOutput = ""       24                  +    if (line.length() == 0 | line.matches("\\s+")) {       25                  +      failedOutput += "Empty Line"       26                  +      returnArray.insert(0, line + "--->" + failedOutput)       27                  +      returnArray.insert(1, passedOutput)       28                  +      return returnArray.toArray       29                  +    } else if (line.split(",").length < 5) {       30                  +      failedOutput += "Record Types Not Found or In sufficient input"       31                  +      returnArray.insert(0, line + "--->" + failedOutput)       32                  +      returnArray.insert(1, passedOutput)       33                  +      return returnArray.toArray       34                  +    } else {       35                  +      val arrString = line.split(",")       36                  +      val arrLength = arrString.length       37                  +      val big4 = arrString.slice(0, 4)       38                  +      val big4String = big4.mkString(",")       39                  +      val partn_nbr = big4.slice(0, 1)(0)       40                  +      val cnsm_id = big4.slice(1, 2)(0)       41                  +      val src_cd = big4.slice(2, 3)(0)       42                  +      val lgcy_src_id = big4.slice(3, 4)(0)       43                  +      val recIds = arrString.slice(4, arrLength)       44                  +      passedOutput = big4String       45                  +      failedOutput = "Big 4:" + big4String       46                  +      //val recTypeExistSet = recTypExist.toSet       47                  +      //val recIdSet = recIds.toSet       48                  +      //var position = 0       49                  +      var failedRecTypes = recIds.filter { recid => !recTypExist.contains(recid) }       50                  +      var passedRecTypes = recIds.filter { recid => recTypExist.contains(recid) }       51                  +      val failedRecTypesString = failedRecTypes.mkString(",")       52                  +      val passedRecTypesString = passedRecTypes.mkString(",")       53                  +      passedOutput += "," + passedRecTypesString       54                  +      if (partn_nbr.matches("\\d+")) {       55                  +        if (partn_nbr.toInt > 20 | partn_nbr.toInt < 1) {       56                  +          failedOutput += "----->Error in Partition Number: " + partn_nbr       57                  +          passedOutput = ""       58                  +        }       59                  +      } else {       60                  +        failedOutput += "----->Error in Partition Number: " + partn_nbr       61                  +        passedOutput = ""       62                  +      }       63                  +      if (cnsm_id.matches("\\d+")) {       64                  +        if (cnsm_id.toInt == 0) {       65                  +          failedOutput += "---->Error in Consumer ID: " + cnsm_id       66                  +          passedOutput = ""       67                  +        }       68                  +      } else {       69                  +        failedOutput += "---->Error in Consumer ID: " + cnsm_id       70                  +        passedOutput = ""       71                  +      }       72                  +      if (lgcy_src_id.matches("\\s+") | (lgcy_src_id.equals("null") | lgcy_src_id.isEmpty())) {       73                  +        failedOutput += "----->Error in Legacy Src Id: " + lgcy_src_id       74                  +        passedOutput = ""       75                  +      }       76                  +      if (src_cd.matches("\\s+") | (src_cd.equals("null") | src_cd.isEmpty())) {       77                  +        failedOutput += "----->Error in Source Code: " + src_cd       78                  +        passedOutput = ""       79                  +      }       80                  +      if (failedRecTypes.length != 0) {       81                  +        failedOutput += "failed Record Types: " + failedRecTypesString       82                  +      }       83                  +      returnArray.insert(0, failedOutput)       84                  +      returnArray.insert(1, passedOutput)       85                  +      return returnArray.toArray       86                  +    }       87                  +  }       88                  +  def cleanOutputPath(outputNodeAddress: String, outputFilePath: String): Unit = {       89                  +    val hadoopConf = new org.apache.hadoop.conf.Configuration()       90                  +    val hdfs = org.apache.hadoop.fs.FileSystem.get(new java.net.URI(outputNodeAddress), hadoopConf)       91                  +    val path = new org.apache.hadoop.fs.Path(outputFilePath)       92                  +    if (hdfs.exists(path)) {       93                  +      println("deleting output directory from path : " + path)       94                  +      try { hdfs.delete(path, true) } catch { case _: Throwable => {} }       95                  +    } else { printf("output directory from path %s is empty.... Exiting clean", path) }       96                  +  }       97                  +       98                  +  def stringreplace(database: String, recordid: String, big4: String, query: String): (String, (String, String)) = {       99                  +    val big4arr = big4.split("~")       100                  +    val partn_nbr = big4arr.slice(0, 1)(0)       101                  +    val cnsm_id = big4arr.slice(1, 2)(0)       102                  +    val src_cd = big4arr.slice(2, 3)(0)       103                  +    val lgcy_src_id = big4arr.slice(3, 4)(0)       104                  +    val finalquery = (recordid, query.replaceAll("%SCHEMA%", database).replaceAll("%CNSM_ID%", cnsm_id).replaceAll("%PARTN_NBR%", partn_nbr).replaceAll("%SRC_CD%", "\\\"" + src_cd + "\\\"").replaceAll("%LGCY_SRC_ID%", "\\\"" + lgcy_src_id + "\\\""))       105                  +    val x = (big4, finalquery)       106                  +    return x       107                  +  } 108                  +} 
